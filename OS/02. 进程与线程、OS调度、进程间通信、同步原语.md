# 进程与线程

## 进程: 任务的运行时的抽象

- 进程的组成
  - 静态部分: 程序运行需要的代码和数据
  - 动态部分: 程序运行期间的状态(如: PC寄存器、堆、栈)

- Process Control Block: 进程的相关数据结构
  - 存放进程相关的各种信息
    - 如: 进程的标识符、内存、打开的文件
  - 保存进程在切换时的状态(上下文)
- 进程的切换
  - <img src="https://s3.ax1x.com/2021/02/06/yY0dSS.png" alt="ProcessSwitching" style="zoom:50%;" />
  - 线程的切换与之类似
- 进程占用的内存空间
  - 代码(线程间共享)
  - 堆(线程间共享)
  - 栈(每个线程拥有自己的栈)
  - 内核栈(每个线程拥有自己的内核栈, 内核栈在内核中)
  - ...

## 线程: 轻量级的任务的运行时的抽象

- 线程只包含运行时所需的最小状态(主要是寄存器和栈), 线程所需的静态部分(如代码)由进程提供
- 线程是OS调度的基本单元、上下文切换的单位
- TLS(Thread Local Storage)
  - 线程库允许定义每个线程独有的数据

## 线程模型

- 用户态线程
  - 内核不可见, 不受内核直接管理
  - 在应用态创建, 线程相关信息主要存放在应用态中
- 内核态线程
  - 内核可见, 受内核直接管理
  - 由内核创建, 线程相关信息存放在内核中
- 用户态线程的执行
  - 用户线程无法直接竞争CPU资源, 用户线程若要想获得CPU资源, 必须先被调度, 以便和某个内核线程绑定, 然后内核线程再去竞争CPU资源, 将得到的CPU资源分配给用户线程

- 线程模型表示了用户态线程与内核态线程之间的联系
  - 多对一模型: 多个用户态线程对应一个内核态线程
    - 优点: 内核管理简单
    - 缺点: 可扩展性差, 无法适应多核机器的发展, 在主流OS中弃用
  - 一对一模型: 一个用户态线程对应一个内核态线程
    - 优点: 解决了多对一模型中的可拓展性问题
    - 缺点: 内核线程数量大, 开销大
    - 主流OS都采用一对一模型(如: Windows、Linux)
  - 多对多模型(Scheduler Activation): 多个用户态线程对应多个内核态线程
    - 优点: 解决了多对一模型的可拓展性问题, 以及一对一模型中内核线程过多的问题
    - 缺点: 管理复杂
    - 多对多模型在虚拟化中得到了广泛的应用

- Thread Control Block: 线程的相关数据结构
  - 一对一模型的TCB可以分为两部分
  - 内核态: 与PCB结构类似
    - 在上下文切换中会使用
  - 应用态: 可以由线程库定义
    - Linux: pthread结构体
    - Windows: TIB(Thread Information Block)
    - 可以认为是内核TCB的扩展

## 纤程

- 一对一模型的局限
  - 一对一模型难以满足一些程序对及时调度的需求
  - 内核线程初始化时间较长、用户态线程上下文切换开销大(因为需要进入内核态), 在执行大量亚毫秒级的线程(如: 处理Web请求)时容易遇到性能瓶颈
- 纤程
  - 纤程是一种比线程更加轻量级的运行时抽象
  - 纤程是特殊的用户态线程
- 纤程的优点
  - 不单独对应内核线程, 创建纤程时不需要额外创建内核态线程
  - 上下文切换快(不需要进入内核态)
  - 允许用户态自主调度, 有助于做出更优的调度决策

- 对纤程的支持

  - Linux对纤程的支持: ucontext
  - Windows对纤程的支持: Fiber库
    - Fiber库支持Fiber Local Storage(FLS)

  - 程序语言对纤程的支持: 协程(coroutine)
    - Go、Python、Lua等语言都支持协程
    - C++自20也开始支持协程

# OS调度

- 为什么需要调度
  - OS中运行的线程的数量远大于CPU核的数量
  - 同一时间CPU核无法同时运行所有的线程, 因此需要OS调度来决定运行哪些线程
- 调度的基本流程
  - (一个CPU核中)上一个线程执行结束
  - 调度器决策需要执行的下一个线程, 以及该线程的执行时长
- 线程执行结束的可能情况
  - 执行时长用尽
  - 等待I/O
  - 主动睡眠
  - 中断
  - ...
- 调度器的目标
  - 降低周转时间: 任务第一次进入系统到执行结束的时间
  - 降低响应时间: 任务第一次进入系统到第一次给用户输出的时间
  - 实时性: 在任务的截止时间内完成任务
  - 公平性: 每个任务都应该有机会执行
  - 低开销: 调度器本身不应该有过多开销
  - 可扩展: 随着任务数量增加, 仍能正常工作
  - ...

## 经典调度方法

- First Come First Served(FCFS)/First In First Out(FIFO)
- Shortest Job First(SJF)
- Preemptive Scheduling(抢占式调度)
  - 每次任务执行一定时间后会被切换到下一个任务
  - 通过定时触发的时钟中断实现
  - Round Robin(RR, 时间片轮转)

## 优先级调度

- Multi-level Queue

  - 将任务按优先级的不同, 分别放入不同的优先级队列中
  - 只有当高优先级队列中的任务执行完毕后, 才会执行低优先级的队列中的任务
  - 同一优先级队列中的任务的执行顺序可以采用FIFO或RR等调度方法

- 应该具有高优先级的任务

  - I/O绑定的任务
    - 目的: 提高资源利用率
  - 用户设置的重要任务
  - 时延要求较高(需要在短时间内完成)的任务
  - 等待时间过长的任务
    - 目的: 提高公平性

- 优先级反转问题

  > 假设三个任务准备执行，A，B，C，优先级依次是A>B>C，其中A和C在执行时需要占用同一资源；
  >
  > 首先：C处于运行状态，获得CPU正在执行，同时占有了资源；
  >
  > 其次：A进入就绪状态，因为优先级比C高，所以获得CPU，A转为运行状态；C进入就绪状态；
  >
  > 第三：执行过程中需要使用资源，而这个资源又被等待中的C占有的，于是A进入阻塞状态，C回到运行状态；
  >
  > 第四：此时B进入就绪状态，因为优先级比C高，B获得CPU，进入运行状态；C又回到就绪状态；
  >
  > 第五：如果这时又出现B2，B3等任务，他们的优先级比C高，但比A低，那么就会出现高优先级任务的A不能执行，反而低优先级的B，B2，B3等任务可以执行的奇怪现象，而这就是优先反转。

- 优先级反转问题的原因

  - OS的调度策略与锁(在这里是资源)的调度策略不匹配导致
  
- 优先级反转问题的解决方案: 调整OS的调度策略

  - 优先级继承
    - 当发现高优先级的任务因为低优先级任务占用资源而阻塞时, 就将低优先级任务的优先级提升到等待它所占有的资源的最高优先级任务的优先级
  - 优先级天花板
    - 将申请某资源的任务的优先级提升到可能访问该资源的所有任务中最高优先级任务的优先级
    - 这个优先级称为该资源的优先级天花板

## 公平共享调度

- 应用场景: 云计算平台多租户共享处理器
  - 在云计算平台中, 计算资源是按价格出售的
  - 租户在意自己获得的CPU资源(CPU时间)
  - 出价相同的租户应该占有相同的CPU资源

- 每个用户占有的资源是成比例的
  - 而非被任务的数量决定
- 每个用户占用的资源是可以被计算的
  - 设定"权重值"以确定相对比例
  - 如: 权重为4的用户使用的资源, 是权重为2的用户的2倍

### Lottery Scheduling

- Ticket
  - 每个用户拥有一定的Ticket
  - 用户可以将Ticket分配给需要执行的任务

- Lottery Scheduling
  - 每次调度时, 生成随机数R(R在0到Ticket总数量之间)
  - 根据R决定下一次需要调度的任务
  - <img src="https://s3.ax1x.com/2021/02/08/yU26Pg.png" alt="Lottery Scheduling" style="zoom:50%;" />

### Stride Scheduling

- Stride Scheduling可以认为是Lottery Scheduling的确定版本
- Stride Scheduling在运行的时长较短时就可保证CPU资源分配的公平, 而Lottery Scheduling只有当时间较长之后才能保证分配的公平
- Stride
  - 根据Ticket的数量为每个任务分配每一步的步长
  - Ticket越小, 步长越长
- 当一个任务被调度执行后, 其总步长会增加自己的一个步长
- 调度器总会选择那个总步长最小的任务进行调度

## 实时调度

- 软实时
  - 从统计的角度来说, 一个任务能够得到有确保的处理时间, 到达系统的事件也能够在截止期限到来之前得到处理, 但违反截止期限并不会带来致命的错误
- 硬实时
  - 指系统要有确保的最坏情况下的服务时间, 即对于事件的响应时间的截止期限是无论如何都必须得到满足的

# 进程间通信

- 独立进程应用程序
  - 一个进程就是一个应用程序
  - 不会影响其他进程, 也不会被其他进程影响
- 独立进程应用程序的问题
  - 功能的实现存在大量重复
    - 如: 都需要访问数据库
  - 没有信息共享
    - 如: 都需要访问一个磁盘数据

- 进程间通信(Inter-process Communication, IPC)
  - 进程协作的达成依赖于进程间通信
  - 通信方式
    - 通过内核或者其他共享资源进行通信

## 通过共享内存通信

- 实现方法
  - OS为多个进程映射共同的内存区域
- 共享内存的同步问题
  - 发送者不能覆盖未被读取的数据
  - 轮询(以检查是否有自己的信息)间隔过短导致资源浪费
  - 轮询间隔过长, 消息接收时延长

## 同步问题的解决: 消息传递

- 直接通信
  - 一个连接唯一对应一对进程
  - 连接可以是双向的, 也可以是单向的

- 间接通信
  - 所有的接收者都订阅一个信箱(所有的接收者都是对等的, 一个消息可以由任何一个接收者处理)
  - 发送者通过信箱向所有的接收者广播消息
  - 接收到消息的接收者就去处理消息
  - 连接可以是双向的, 也可以是单向的

- 通信连接的缓冲
  - 通信连接可以选择保留还没有处理的消息
  - 零容量: 通信连接本身不缓冲消息, 发送者只能阻塞等待接受者接收消息
  - 有限容量: 连接可以缓冲有限个消息, 当缓存区满之后发送者只能阻塞等待
  - 无限容量: 连接可以缓冲任意数量的消息, 发送者几乎不需要等待

## Unix管道

- 管道是Unix等宏内核系统中非常重要的进程间通信的机制
  - 如: 常见的命令`ls | grep`
- 管道的特点
  - 单向通信, 当缓冲区满时阻塞
  - 数据不带类型, 即字节流

## 轻量级远程调用(LRPC)

- Lightweight Remote Procedure Call
- LRPC需要解决的两个主要问题
  - 控制流转换: 由调用者进程快速转到被调用者进程
  - 数据传输: 将栈和寄存器参数传递给被调用者进程

### 控制流转换: 调度导致不确定时延

- 控制流转换时需要下陷到内核
- 内核系统为了保证公平, 会在内核中根据情况进行调度
  - 在被调用者被调度之前可能会执行多个不相关进程

- 解决方案: 将调用者运行在被调用上下文
  - 只切换地址空间、权限表等状态, 不做调度和真正的线程切换

# 同步原语

## 临界区问题

- 多处理器与多核
  - 单核性能遇到瓶颈, 不能通过一味提高频率来获得更好的性能
  - 因此CPU厂商希望通过提高CPU核的数量来提高CPU性能
- 多核的问题
  - 正确性问题: 多核难以管理, 容易出现并发问题(如: 一个核在读这个数据, 另一个核在写这个数据)
  - 性能可扩展性问题: 同一时间很难让所有的核都发挥作用

- 竞争条件(Race Condition)
  - 当多个进程同时对共享的数据进行操作
  - 该共享数据最后的结果依赖于这些进程特点的执行顺序
- 临界区问题
  - 如何让多个访问者在同一时间只能有一个访问者在临界区中访问数据
- 解决临界区问题的三个必要条件
  - 互斥访问: 在同一时刻, 有且仅有一个进程可以进入临界区
  - 有限等待: 当一个线程申请进入临界区之后, 必须在有限的时间内获得许可进入临界区
  - 空闲让进: 当没有进程在临界区中时, 必须在申请进入临界区的进程中选择一个进入临界区

## 临界区问题的软件解决方案: Peterson算法

```c
int flag[2];
int turn;

void enterRegion(int process) { // process must be 0 or 1
    int other = 1 - process;
    flag[process] = 1;
    turn = other;
    while (flag[other] && turn == other);
}
void leaveRegion(int process) {
    flag[process] = 0;
}
```

- Peterson算法对临界区问题三个条件的满足情况
  - 互斥访问: 满足
  - 有限等待: 不满足
    - 存在CPU打断, 即当CPU每次都在特定的时刻打断while条件判断时, 等待时间无限
  - 空闲让进: 满足

## 临界区问题的硬件解决方案: 硬件原子操作

- 原子操作

  - 执行期间不可被打断的操作集合
  - 其他核心不会看到执行时的中间状态(all-or-nothing)
  - 原子操作一般包括: CAS、TAS、TTAS、FAA

- CAS(Compare And Swap)

  ```c
  int CAS(int *addr, int expected, int newValue) {
      int tmp = *addr;
      if (*addr == expected) {
          *addr = newValue;
      }
      return tmp;
  }
  ```

  - CAS语义: 比较指定内存中的值是否与预期的值相同, 若相同则将内存中的值修改为新的值

- CAS在硬件上的实现

  - 在Intel CPU中的实现方式: 锁总线
  - 在ARM CPU中的实现方式: LL/SC(Load-linked&Store-conditional)

- FAA(Fetch And Add)

  ```c
  int FAA(int *addr, int incre) {
      int tmp = *addr;
      *addr += incre;
      return tmp;
  }
  ```

### Spinlock(自旋锁)

```c
int *lock = ...; // 一个指针, 其指向的值为0表示没有被申请, 为1表示已被申请

void lock() {
    while (CAS(lock, 0, 1) != 0); // Busy looping
}
void unlock() {
    *lock = 0;
}
```

- 自旋锁对临界区问题三个条件的满足情况
  - 互斥访问: 满足
  - 有限等待: 不满足
    - 若每次CAS都竞争失败则等待时间无限
  - 空闲让进: 依赖于硬件的实现
    - 若当多个CPU核同时对一个地址执行操作时, 能保证至少一个能成功, 则可以满足空闲让进

### Ticket Lock(排号锁)

```c
int *owner = ...; // 一个指针, 其指向的值为锁的当前持有者
int *next = ...; // 一个指针, 其指向的值为当前放号的最新值

void lock() {
    myTicket = FAA(next, 1);
    while (*owner != myTicket); // Busy looping
}
void unlock() {
    *owner++;
}
```

- 排号锁对临界区问题三个条件的满足情况
  - 互斥访问: 满足
  - 有限等待: 满足
    - 排号锁是一种相对公平的锁, 只要拿上号了, 就不会有竞争失败的情况
  - 空闲让进: 依赖于硬件的实现
    - 若当多个CPU核同时对一个地址执行操作时, 能保证至少一个能成功, 则可以满足空闲让进

## 读写锁

- 读锁和写锁
  - 当一个线程只是去读取数据时, 为数据加读锁
  - 当一个线程需要修改数据时, 为数据加写锁
  - 读锁与读锁之间不互斥, 读锁与写锁之间互斥, 写锁与写锁之间互斥
- 读写锁的偏向性
  - 偏向性考虑的问题
    - t0时刻: 有读者在临界区
    - t1时刻: 有新的读者在等待
    - t2时刻: 另一个读者希望进入临界区
  - 允许t2时刻的读者进入临界区
    - 偏向于更好的并行性
  - 不允许t2时刻的读者进入临界区
    - 偏向于更好的公平性
- 读写锁相较于一般的锁, 并行性更佳, 带来的性能更好

## RCU(Read Copy Update)

- 利用硬件提供的原子操作, 写者在修改数据时不需要为数据加锁, 也可以保证读者不会读取到修改数据时的中间结果(因此读者在读取时也不需要上锁)
- RCU的实现
  - 由于硬件原子操作, 其能够一次性操作的数据有大小限制(最大128bit), 因此不能直接使用硬件原子操作来修改数据
  - 因此一般利用硬件原子操作来修改某个关键的指针, 在该指针修改前, 读者读取到的是修改前数据, 在该指针修改后, 读者读取到的是修改后的数据
- RCU vs 读写锁
  - RCU中读者和写者均不需要上锁, 因此性能比读写锁更佳
  - RCU中写者为了实现不加锁, 需要进行大量的操作(如copy数据), 逻辑繁琐, 开销较大
  - RCU一般只考虑在对性能要求很高, 或提升性能带来的收益很大的地方使用

## 管程(Monitor)

- 管程提供一些列由程序语言实现的thread-safe的接口, 开发者可以直接使用这些接口
- 管程实现了在一个时间点, 最多只有一个线程在执行管程的某个子程序

## 死锁

- 死锁解决方案
  - 出问题时再处理: 死锁的检测与恢复
  - 设计时避免: 死锁预防
  - 运行时避免死锁: 死锁避免

### 死锁预防

- 避免互斥访问
- 不允许持有并等待
  - 当持有一部分锁但是无法申请到另一部分锁时, 释放所有的锁
  - 引起的新问题: 活锁
    - 所有的进程都在不停地申请并释放锁
    - 活锁的问题的可以随尝试的次数而解决
- 资源允许抢占
  - 需要考虑如何恢复
- 打破循环等待
  - 按特定顺序获取资源: 所有资源进行编号, 所有线程递增获取
  - 难点: 难以在一开始就知道有哪些资源需要被获取

### 死锁避免

- 银行家算法
  - 所有线程获取资源需要经过管理者同意
  - 管理者预演是否会造成死锁, 若会, 则阻塞线程, 等到不会造成死锁时再给线程

## 优先级反转问题

- 正如在"OS调度"中所述, OS的调度策略与锁的调度策略不协调会引起线程的优先级反转问题
  - 即低优先级的线程可能会比高优先级的线程先执行
- 解决优先级反转问题的方案: 调整OS的调度策略
  - 不可打断临界区协议(Non-preemptive Critical Sections Protocol, NCP)
    - 进入临界区后不允许其他进程打断, 即禁止OS调度
  - 优先级继承协议(Priority Inheritance Protocol, PIP)
    - 高优先级进程被阻塞时, 让锁的持有者继承自己的优先级
  - 即时优先级置顶协议(Immediate Priority Ceiling Protocols, IPCP)
    - 获取锁时, 给锁持有者该锁的可能竞争者中的最高优先级
  - 原生优先级置顶协议(Original Priority Ceiling Protocols, OPCP)
    - 高优先级进程被阻塞时, 给锁持有者该锁的可能竞争者中的最高优先级

## 多核情况下的缓存一致性

- 单核CPU的多级缓存结构
  - <img src="https://s3.ax1x.com/2021/02/14/ysL0MQ.png" alt="Single Core" style="zoom:50%;" />

- 多核CPU的多级缓存结构
  - <img src="https://s3.ax1x.com/2021/02/14/ysLTd1.png" alt="Multi Core" style="zoom: 50%;" />
  - 每个核心有自己的私有高速缓存(L1 Cache)
  - 多个核心共享一个二级高速缓存(L2 Cache)
  - 所有核心共享一个末级高速缓存(L3 Cache, LLC)
- 缓存一致性
  - 保证不同CPU核对同一地址的值达成共识

### 窥探式/目录式缓存一致性协议

- 窥探式/目录式缓存一致性协议是用于确保缓存一致性的协议

- MSI状态迁移
  - Modified(独占修改)状态
    - 当CPU核进行本地写时, 被写的缓存行(Cache Line)在该CPU中迁移到Modified状态
  - Invalid状态
    - 当一个CPU核进行本地写时, 其他CPU核的对应缓存行迁移到Invalid状态
  - Share状态
    - 当该CPU的某一缓存行处于Invalid状态时, 读取该缓存行的数据, 该CPU核会向拥有该缓存行的最新数据的CPU核那里获取该缓存行数据, 两个CPU核的该缓存行迁移到Share状态

## 多核环境性能瓶颈

- 性能瓶颈的来源
  - CPU核越多, 就需要CPU核花费更多的精力来保证缓存的一致性
    - 如: 一个CPU核将一个临界区加锁后, 所有竞争这个临界区的CPU核都会向这个CPU核获取最新信息
  - 因此CPU核过多后会出现性能不增反降的情况

### Back-off策略

- Back-off策略着重于减轻CPU核对单一缓存行的竞争, 以减轻维护缓存一致性的开销
- 做法: 在每次尝试加锁失败后进行一个Back-off, 即等待一段时间再尝试加锁
- Back-off是一个治标不治本的解决方案

### MCS锁

- MCS锁的性能瓶颈解决策略
  - 多核性能瓶颈的主要来源是多个CPU核高度竞争(访问)同一个缓存行
  - MCS锁可以让不同的竞争者访问不同的缓存行, 从而避免对某一缓存行的过多的一致性维护
- MCS锁的等待队列
  - MCS锁的竞争者会排成一个队列, 每个竞争者会有一个等待状态, 标志着自己是否可以获得锁(GRANTED/WATING), 上一个竞争者会持有下一个竞争者的等待状态的指针
  - 队列的队头为获得锁的竞争者, 只有这个竞争者的等待状态为GRANTED, 当这个竞争者释放锁时, 就会将其后一个竞争者的等待状态改为GRANTED
  - 当有新的竞争者来申请锁时, 会将自己作为等待队列的队尾, 并将自己的等待状态的指针交给上一个竞争者
- 等待队列如何解决性能瓶颈
  - 等待队列中没获得锁的竞争者都只会不断访问自己的状态, 查看自己的状态是否被改为了GRANTED
  - 这样就避免了竞争者都访问同一个缓存行的问题

## 非一致内存访问

- 非一致内存访问(Non Uniform Memory Access, NUMA)技术是一种用于多处理器的电脑内存体设计

- NUMA节点
  - 在多处理器机器中, 一个CPU(或多个CPU核)与多个内存条会组成一个NUMA节点
  - 不同的NUMA节点之间通过内部的互联总线进行连接
- NUMA技术带来的挑战
  - 当CPU核访问其他的NUMA节点中的内存条存储的数据时, 其访问速度以及带宽都会下降

### NUMA-aware设计 -- 以Cohort锁为例

- Cohort的全局锁和本地锁
  - 全局锁的竞争者是NUMA节点
  - 本地锁的竞争者是NUMA节点中的CPU核
  - 当一个NUMA节点竞争到全局锁后, 该节点中的CPU核会竞争本地锁, 竞争到本地锁的CPU核就可以进入临界区
- Cohort锁就相当于将MCS锁中的竞争者按照NUMA节点进行了排序, 让同一个NUMA节点中的竞争者集中执行

## 内存模型

- 严格一致性模型
  - 保证读-读、读-写、写-读、写-写的顺序与代码一致
- 顺序一致性模型
  - 保证读-读、读-写、写-读、写-写的顺序与代码一致
- TSO(Total Store Ordering)一致性模型
  - 保证读-读、读-写、写-写的顺序与代码一致, 但不保证写-读的顺序一致
  - TSO一致性不保证写-读的顺序一致的原因
    - 在CPU核和Cache之间有一层Load/Store Buffer
    - Load/Store Buffer可能会将CPU核写给Cache的数据延迟一段时间, 等写的数据多了在发送, 以提高写的效率
    - 因此写操作的执行结果可能被延后

- 弱一致性模型
  - 不保证读-读、读-写、写-读、写-写的顺序与代码一致